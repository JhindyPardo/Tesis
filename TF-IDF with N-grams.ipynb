{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pued tard': 0.0, 'justici reparacion': 0.028881132523331052, 'd proces': 0.028881132523331052, 'justici transicional': 0.0, 'derech d': 0.028881132523331052, 'victim d': 0.028881132523331052, 'victim verd': 0.028881132523331052, 'pod congres': 0.028881132523331052, 'ordinari si': 0.0, 'anos d': 0.028881132523331052, 'proces pued': 0.0, 'proces q': 0.028881132523331052, 'defiend dar': 0.028881132523331052, 'calvari victim': 0.028881132523331052, 'd calvari': 0.028881132523331052, 'justici ordinari': 0.0, 'verd justici': 0.028881132523331052, 'dar 3': 0.028881132523331052, 'firm d': 0.028881132523331052, '3 anos': 0.028881132523331052, 'congres pod': 0.028881132523331052, 'd dej': 0.028881132523331052, 'pod mont': 0.028881132523331052, 'vulner derech': 0.028881132523331052, 'd victim': 0.028881132523331052, 'q vulner': 0.028881132523331052, 'dej band': 0.028881132523331052, 'nivel impun': 0.0, 'impun altos': 0.0, 'altos proces': 0.0, 'anos comparacion': 0.0, 'mont defiend': 0.028881132523331052, 'comparacion justici': 0.0, 'tard anos': 0.0, 'band pod': 0.028881132523331052, 'si nivel': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "#LEER EL DATASET\n",
    "\n",
    "df=pd.read_csv('Dataset.csv')\n",
    "#L=np.asarray(df)\n",
    "#print(L)\n",
    "\n",
    "data = pd.read_csv(\"Dataset.csv\", sep=\",\")\n",
    "Tweet = data[[\"Tweet\"]]\n",
    "T=np.asarray(Tweet)\n",
    "\n",
    "##########################################\n",
    "punctuation = list(string.punctuation)\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stop = stopwords.words('spanish') + punctuation\n",
    "#############Documentos##################\n",
    "documentA = str(T[55])\n",
    "documentB = str(T[57])\n",
    "########################################\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # :(\n",
    "        [oO\\-]? # No se\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # \n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "##############Metodos####################\n",
    "\n",
    "\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    # Romper la oración en el tokens\n",
    "    tokens_first = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE )\n",
    "    tokens = tokens_first.findall(s)\n",
    "        \n",
    "    # eliminar tokens vacíos, links, menciones, stopwords y hastags\n",
    "    tokens = [token for token in tokens if (token != \"\" ) and (token not in stop) and (not token.startswith(('#', '@', 'https:', 'http:')))]\n",
    "    #lematizar\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # print(tokens)\n",
    "    # funcion zip genera n-grams\n",
    "    # Concatena los tokens en los ngrams y los retorna\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "#########################################\n",
    "\n",
    "##########Preprocesamiento##########\n",
    "n=2\n",
    "bagOfWordsA = generate_ngrams(documentA, n)\n",
    "# print(bagOfWordsA)\n",
    "bagOfWordsB = generate_ngrams(documentB, n)\n",
    "# print(bagOfWordsB)\n",
    "\n",
    "\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "# print(numOfWordsA)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "# print( numOfWordsA)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "###################################\n",
    "\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "# print(tfA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "#print(tfB)              \n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "\n",
    "                \n",
    "                \n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "print(tfidfA)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(df)\n",
    "                \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
