{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['firm d', 'd proces', 'proces q', 'q vulner', 'vulner derech', 'derech d', 'd victim', 'victim d', 'd dej', 'dej band', 'band pod', 'pod congres', 'congres pod', 'pod mont', 'mont defiend', 'defiend dar', 'dar 3', '3 anos', 'anos d', 'd calvari', 'calvari victim', 'victim verd', 'verd justici', 'justici reparacion']\n",
      "['justici ordinari', 'ordinari si', 'si nivel', 'nivel impun', 'impun altos', 'altos proces', 'proces pued', 'pued tard', 'tard anos', 'anos comparacion', 'comparacion justici', 'justici transicional']\n",
      "{'d dej': 0, 'q vulner': 0, 'band pod': 0, 'justici transicional': 0, 'tard anos': 0, 'd proces': 0, 'proces q': 0, 'victim d': 0, 'anos d': 0, 'nivel impun': 0, 'pod mont': 0, 'derech d': 0, 'justici ordinari': 0, 'victim verd': 0, 'verd justici': 0, 'proces pued': 0, 'd victim': 0, 'mont defiend': 0, 'calvari victim': 0, 'vulner derech': 0, 'justici reparacion': 0, 'pued tard': 0, '3 anos': 0, 'anos comparacion': 0, 'si nivel': 0, 'congres pod': 0, 'd calvari': 0, 'impun altos': 0, 'ordinari si': 0, 'comparacion justici': 0, 'defiend dar': 0, 'dej band': 0, 'firm d': 0, 'pod congres': 0, 'altos proces': 0, 'dar 3': 0}\n",
      "{'d dej': 1, 'q vulner': 1, 'band pod': 1, 'justici transicional': 0, 'tard anos': 0, 'd proces': 1, 'proces q': 1, 'victim d': 1, 'anos d': 1, 'nivel impun': 0, 'pod mont': 1, 'derech d': 1, 'justici ordinari': 0, 'victim verd': 1, 'verd justici': 1, 'proces pued': 0, 'd victim': 1, 'mont defiend': 1, 'calvari victim': 1, 'vulner derech': 1, 'justici reparacion': 1, 'pued tard': 0, '3 anos': 1, 'anos comparacion': 0, 'si nivel': 0, 'congres pod': 1, 'd calvari': 1, 'impun altos': 0, 'ordinari si': 0, 'comparacion justici': 0, 'defiend dar': 1, 'dej band': 1, 'firm d': 1, 'pod congres': 1, 'altos proces': 0, 'dar 3': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "#LEER EL DATASET\n",
    "\n",
    "df=pd.read_csv('Dataset.csv')\n",
    "#L=np.asarray(df)\n",
    "#print(L)\n",
    "\n",
    "data = pd.read_csv(\"Dataset.csv\", sep=\",\")\n",
    "Tweet = data[[\"Tweet\"]]\n",
    "T=np.asarray(Tweet)\n",
    "\n",
    "##########################################\n",
    "punctuation = list(string.punctuation)\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stop = stopwords.words('spanish') + punctuation\n",
    "#############Documentos##################\n",
    "documentA = str(T[55])\n",
    "documentB = str(T[57])\n",
    "########################################\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # :(\n",
    "        [oO\\-]? # No se\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # \n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "##############Metodos####################\n",
    "\n",
    "\n",
    "\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    # Romper la oración en el tokens\n",
    "    tokens_first = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE )\n",
    "    tokens = tokens_first.findall(s)\n",
    "        \n",
    "    # eliminar tokens vacíos, links, menciones, stopwords y hastags\n",
    "    tokens = [token for token in tokens if (token != \"\" ) and (token not in stop) and (not token.startswith(('#', '@', 'https:', 'http:')))]\n",
    "    #lematizar\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    # print(tokens)\n",
    "    # funcion zip genera n-grams\n",
    "    # Concatena los tokens en los ngrams y los retorna\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "#########################################\n",
    "\n",
    "##########Preprocesamiento##########\n",
    "n=2\n",
    "bagOfWordsA = generate_ngrams(documentA, n)\n",
    "print(bagOfWordsA)\n",
    "bagOfWordsB = generate_ngrams(documentB, n)\n",
    "print(bagOfWordsB)\n",
    "\n",
    "\n",
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))\n",
    "\n",
    "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
    "print(numOfWordsA)\n",
    "for word in bagOfWordsA:\n",
    "    numOfWordsA[word] += 1\n",
    "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "print( numOfWordsA)\n",
    "for word in bagOfWordsB:\n",
    "    numOfWordsB[word] += 1\n",
    "###################################\n",
    "\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "#print(tfA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "#print(tfB)              \n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "                \n",
    "                \n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "# with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#     print(df)\n",
    "                \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
