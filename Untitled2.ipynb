{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "import re\n",
    "\n",
    "#LEER EL DATASET\n",
    "data = pd.read_csv(\"Dataset5.csv\", sep=\",\")\n",
    "##########################################\n",
    "punctuation = list(string.punctuation)\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stop = stopwords.words('spanish') + punctuation\n",
    "vc = TfidfVectorizer()\n",
    "#############Documentos##################\n",
    "Tweet = data[\"Tweet\"]\n",
    "Label = data[\"Label\"]\n",
    "T=np.asarray(Tweet)\n",
    "bigNumOfWords = []\n",
    "bigBagOfWords = []\n",
    "unique = []\n",
    "bigTf = {}\n",
    "bigTfidf= []\n",
    "########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # :(\n",
    "        [oO\\-]? # No se\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # \n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)', # anything else\n",
    "    r'[0-9]+'\n",
    "]\n",
    "##############Metodos####################\n",
    "\n",
    "####Quitar numeros##############\n",
    "def remove(list): \n",
    "    pattern = '[0-9]'\n",
    "    list = [re.sub(pattern, '', i) for i in list]\n",
    "    return list\n",
    "#######################################\n",
    "def generate_ngrams(s, n):\n",
    "    # Convert to lowercases\n",
    "    s = s.lower()\n",
    "    # Romper la oración en el tokens\n",
    "    tokens_first = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE )\n",
    "    tokens = tokens_first.findall(s)\n",
    "        \n",
    "    # eliminar tokens vacíos, links, menciones, stopwords y hastags\n",
    "    tokens = [token for token in tokens if (token != \"\" or token != '' ) and (token not in stop) and (not token.startswith(('#', '@', 'https:', 'http:','<')))]\n",
    "    #lematizar\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    tokens = remove(tokens)\n",
    "    tokens = [token for token in tokens if (token != \"\" or token != ''  ) and (token not in stop) and (not token.startswith(('#', '@', 'https:', 'http:')))]\n",
    "#     print(tokens)\n",
    "    # funcion zip genera n-grams\n",
    "    # Concatena los tokens en los ngrams y los retorna\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        if bagOfWordsCount != 0:\n",
    "            tfDict[word] = count / float(bagOfWordsCount)\n",
    "        else:\n",
    "            tfDict[word] = 0\n",
    "\n",
    "    return tfDict\n",
    "\n",
    "\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    \n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            \n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        if val != 0:\n",
    "            idfDict[word] = math.log(N / float(val))\n",
    "        else:\n",
    "            idfDict[word] = 0\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########Preprocesamiento##########\n",
    "n=1\n",
    "count = 0\n",
    "count2 = 0\n",
    "for tweet in T:\n",
    "    bagOfWords = generate_ngrams(str(tweet), n)\n",
    "    unique = set(unique).union(set(bagOfWords))\n",
    "    bigBagOfWords.append(bagOfWords)\n",
    "############Contar Palabras######################\n",
    "for tweet in bigBagOfWords:\n",
    "    numOfWords = dict.fromkeys(unique, 0)\n",
    "    for word in tweet:\n",
    "        numOfWords[word] += 1\n",
    "    bigNumOfWords.append(numOfWords)\n",
    "\n",
    "count = 0\n",
    "##################TF##########################\n",
    "for tweet in T:\n",
    "    tf = computeTF(bigNumOfWords[count], bigBagOfWords[count])\n",
    "    bigTf[count] = tf\n",
    "    count += 1\n",
    "#####################IDF###################\n",
    "idfs = computeIDF(bigNumOfWords)\n",
    "###################TFIDF###################\n",
    "\n",
    "for tweet in T:\n",
    "    tfidf = computeTFIDF(bigTf[count2], idfs)\n",
    "    bigTfidf.append(tfidf)\n",
    "    count2 += 1\n",
    "#################results############################\n",
    "df = pd.DataFrame(bigTfidf)\n",
    "df['Label'] = Label\n",
    "# print(df['Label'])\n",
    "if n == 1:\n",
    "    df.to_csv (r'Features unigrama.csv', index = False, header=True)\n",
    "else:\n",
    "    df.to_csv (r'features bigram.csv', index = False, header=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
